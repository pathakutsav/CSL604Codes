Experiment No. 5-: Token Separation using LEX

AIM-: To write a Lex Specification for token separation using regular expressions.
DESCRIPTION-: Lex helps write programs whose control flow is directed by instances of regular
expressions in the input stream. It is well suited for editor-script type transformations and for
segmenting input in preparation for a parsing routine. Lex source is a table of regular expressions and
corresponding program fragments.

The table is translated to a program which reads an input stream, copying it to an output stream and
partitioning the input into strings which match the given expressions. As each such string is
recognized the corresponding program fragment is executed. The recognition of the expressions is
performed by a deterministic finite automaton generated by Lex. The program fragments written by
the user are executed in the order in which the corresponding regular expressions occur in the input
stream.

Lex turns the user's expressions and actions (called source in this memo) into the host general-purpose
language; the generated program is named yylex. The yylex program will recognize expressions in a
stream (called input in this memo) and perform the specified actions for each expression as it is
detected.

The general format of Lex source is:
{definitions}
%%
{rules}
%%
{user subroutines}

ALGORITHM-:
1. Start.
2. Define the required variables.
3. Define the regular expressions.
4. Define the required auxiliary definitions.
5. Compile the program by using the following commands vi sample.l
    a. lex sample.l ->lex.yy.c
    b. cc lex.yy.c ->./a.out
6. Execute the program by the following command $>> ./a.out 
